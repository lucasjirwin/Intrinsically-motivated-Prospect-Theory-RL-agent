{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuTcL5S14a13v9e0wjfHd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasjirwin/Meta-Learning-aspiration-RL-agent/blob/main/RL_aspiration_metaLearner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Senior Thesis Project:** Developing a meta-learning RL agent which sets its own aspiration level \n",
        "\n",
        "\n",
        "\n",
        "*   **DeepGridWorld class:** contains the grid-world environment with methods for taking a step in the environment, getting available actions and rewards, checking if a state is terminal and printing the agent on the map.\n",
        "*   **DeepQ_Agent class:** contains the Q-learning agent with methods for choosing an action, updating the Q-table (learning), and getting the aspiration.  \n",
        "*   **play function:** Runs iterations of the agent on the environment and updates the Q-table if required. \n",
        "\n",
        "\n",
        "**TODO:**\n",
        "*   **Meta-learner:** PyTorch module, backpropagation through the Q_table (modelled as a tensor). Inputs: aspiration, weight_1, weight_2. Outputs: average cumulative reward \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ljiHsJtQ4THk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "3MiWm0wlDhFE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EM5mEXGRBggC"
      },
      "outputs": [],
      "source": [
        "class DeepGridWorld:\n",
        "    ## Initialise starting data\n",
        "    def __init__(self):\n",
        "        # Set information about the gridworld\n",
        "        self.height = 5\n",
        "        self.width = 5\n",
        "        self.grid = np.zeros(self.height * self.width) -1\n",
        "        \n",
        "        # Set random start location for the agent \n",
        "        self.current_location = (np.random.randint(20,24)) #e1\n",
        "        \n",
        "        # Set locations for the bomb and the gold\n",
        "        self.bomb_location = 8 # e2\n",
        "        self.gold_location = 3 # e3\n",
        "        self.terminal_states = [self.bomb_location, self.gold_location]\n",
        "        \n",
        "        # Set grid rewards for special cells\n",
        "        self.grid[self.bomb_location] = -10\n",
        "        self.grid[self.gold_location] = 10\n",
        "        \n",
        "        # Set available actions\n",
        "        self.actions = [0, 1, 2, 3] # e4\n",
        "    \n",
        "        \n",
        "    ## Put methods here:\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Returns possible actions\"\"\"\n",
        "        return self.actions\n",
        "    \n",
        "    def agent_on_map(self):\n",
        "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
        "        grid = np.zeros(( self.height, self.width))\n",
        "        grid[self.current_location] = 1 #e5\n",
        "        return grid\n",
        "    \n",
        "    def get_reward(self, new_location):\n",
        "        \"\"\"Returns the reward for an input position\"\"\"\n",
        "        return self.grid[new_location] # e6\n",
        "        \n",
        "    \n",
        "    def make_step(self, action):\n",
        "        \"\"\"Moves the agent in the specified direction. If agent is at a border, agent stays still\n",
        "        but takes negative reward. Function returns the reward for the move.\"\"\"\n",
        "        # Store previous location\n",
        "        last_location = self.current_location\n",
        "        \n",
        "        # UP\n",
        "        if action == 0: # e7\n",
        "            # If agent is at the top, stay still, collect reward\n",
        "            if last_location < self.width: #e8 \n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location - self.width) # e9\n",
        "                reward = self.get_reward(self.current_location)\n",
        "        \n",
        "        # DOWN\n",
        "        elif action == 1:\n",
        "            # If agent is at bottom, stay still, collect reward\n",
        "            if last_location >= ((self.width * self.height) - self.width): #e10\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location + self.width) # e11\n",
        "                reward = self.get_reward(self.current_location)\n",
        "            \n",
        "        # LEFT\n",
        "        elif action == 2:\n",
        "            # If agent is at the left, stay still, collect reward\n",
        "            if last_location % self.height == 0: # TODO e12\n",
        "                reward = self.get_reward(last_location) \n",
        "            else:\n",
        "                self.current_location = (self.current_location - 1) # e13\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        # RIGHT\n",
        "        elif action == 3:\n",
        "            # If agent is at the right, stay still, collect reward\n",
        "            if last_location % self.height == self.height - 1: # TODO e14\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location + 1) # e15\n",
        "                reward = self.get_reward(self.current_location)\n",
        "                \n",
        "        return reward\n",
        "    \n",
        "    def check_state(self):\n",
        "        \"\"\"Check if the agent is in a terminal state (gold or bomb), if so return 'TERMINAL'\"\"\"\n",
        "        if self.current_location in self.terminal_states:\n",
        "            return 'TERMINAL'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent():        \n",
        "    # Choose a random action\n",
        "    def choose_action(self, available_actions):\n",
        "        \"\"\"Returns a random choice of the available actions\"\"\"\n",
        "        return np.random.choice(available_actions)   "
      ],
      "metadata": {
        "id": "tMlAwzH5DbVO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQ_Agent():\n",
        "    # Intialise\n",
        "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1, aspiration = 0, weight_1 = 1, weight_2 = 1): # add default aspiration and weights\n",
        "        self.environment = environment\n",
        "        # self.q_table = dict() # Store all Q-values in dictionary of dictionaries \n",
        "        # for x in range(environment.height): # Loop through all possible grid spaces, create sub-dictionary for each\n",
        "        #     for y in range(environment.width):\n",
        "        #         self.q_table[(x,y)] = {'UP':0, 'DOWN':0, 'LEFT':0, 'RIGHT':0} # Populate sub-dictionary with zero values for possible moves\n",
        "\n",
        "        self.q_table = torch.zeros((environment.height*environment.width, 4)) #q1\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.aspiration = aspiration \n",
        "        self.weight_1 = weight_1\n",
        "        self.weight_2 = weight_2\n",
        "    \n",
        "    # Choose the action e-greedily \n",
        "    def choose_action(self, available_actions):\n",
        "        \"\"\"Returns the optimal action from Q-Value table. If multiple optimal actions, chooses random choice.\n",
        "        Will make an exploratory random action dependent on epsilon.\"\"\"\n",
        "        if np.random.uniform(0,1) < self.epsilon:\n",
        "            action = available_actions[np.random.randint(0, len(available_actions))]\n",
        "        else:\n",
        "            q_values_of_state = self.q_table[self.environment.current_location] # change to 1D (e1 in env)\n",
        "            maxValue = torch.max(q_values_of_state) # q2\n",
        "            #action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue]) # tricky index of torch.max value and random choice\n",
        "            res = (q_values_of_state == maxValue).nonzero().numpy() # q3 get indices of highest value actions and convert to np array\n",
        "            action = np.random.choice(res[:,0]) # choose the action randomly if there is a tie (slice for 1D array)\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    # Update the Q-table based on the current observation \n",
        "    def learn(self, old_state, reward, new_state, action):\n",
        "        \"\"\"Updates the Q-value table using Q-learning\"\"\"\n",
        "        q_values_of_state = self.q_table[new_state] # index into table self.q_table[] single index numbers not tuples\n",
        "        max_q_value_in_new_state = torch.max(q_values_of_state) # q4\n",
        "        current_q_value = self.q_table[old_state][action]\n",
        "        # define reward with aspiration = w_1 * reward + w_2 * (reward - aspiration)\n",
        "        reward = self.get_aspiration(reward)\n",
        "        self.q_table[old_state][action] = (1 - self.alpha) * current_q_value + self.alpha * (reward + self.gamma * max_q_value_in_new_state)\n",
        "    \n",
        "    # Transform the reward function using the aspiration formula \n",
        "    def get_aspiration(self, reward):\n",
        "      return self.weight_1 * reward + self.weight_2 * (reward - self.aspiration)\n"
      ],
      "metadata": {
        "id": "8WY2L3fbDds8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play(environment, agent, trials=500, max_steps_per_episode=1000, learn=False):\n",
        "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "    reward_per_episode = [] # Initialise performance log\n",
        "    \n",
        "    for trial in range(trials): # Run trials\n",
        "        cumulative_reward = 0 # Initialise values of each game\n",
        "        step = 0\n",
        "        game_over = False\n",
        "        while step < max_steps_per_episode and game_over != True: # Run until max steps or until game is finished\n",
        "            old_state = environment.current_location\n",
        "            action = agent.choose_action(environment.actions) \n",
        "            reward = environment.make_step(action)\n",
        "            new_state = environment.current_location\n",
        "            \n",
        "            if learn == True: # Update Q-values if learning is specified\n",
        "                agent.learn(old_state, reward, new_state, action)\n",
        "                \n",
        "            cumulative_reward += reward\n",
        "            step += 1\n",
        "            \n",
        "            if environment.check_state() == 'TERMINAL': # If game is in terminal state, game over and start next trial\n",
        "                environment.__init__()\n",
        "                game_over = True     \n",
        "                \n",
        "        reward_per_episode.append(cumulative_reward) # Append reward for current trial to performance log\n",
        "        \n",
        "    return reward_per_episode # Return performance log"
      ],
      "metadata": {
        "id": "yeqM3aSVDqFg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environment = DeepGridWorld()\n",
        "agent = DeepQ_Agent(environment, aspiration = 0.08)\n",
        "av_reward = 0 \n",
        "for iteration in range(100):\n",
        "  reward_per_episode = play(environment, agent, trials = 500, learn = True)\n",
        "\n",
        "  #plt.plot(reward_per_episode)\n",
        "  av_reward += sum(reward_per_episode)\n",
        "\n",
        "print(av_reward/100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tDNCiPTDqxd",
        "outputId": "1984122d-0dec-4861-d135-94d5acb9f29a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2171.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing aspiration (100 iterations, 500 trials)  \n",
        "2184.75, aspiration = 0.5 \\\\\n",
        "2182.38       , aspiration = 0.2 \\\\\n",
        "2195.28      , aspiration = 0.08 \\\\\n",
        "2181.2      , aspiration = 0.05 \\\\\n",
        "2188.52, aspiration = 0 \n"
      ],
      "metadata": {
        "id": "uiFoTCYMOEGz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eu1QVXR7EEq-"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}