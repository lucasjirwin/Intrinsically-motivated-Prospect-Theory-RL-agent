{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasjirwin/Meta-Learning-aspiration-RL-agent/blob/main/RL_aspiration_metaLearner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljiHsJtQ4THk"
      },
      "source": [
        "# **Senior Thesis Project:** Developing a meta-learning RL agent which sets its own aspiration level \n",
        "\n",
        "\n",
        "\n",
        "*   **DeepGridWorld class:** contains the grid-world environment with methods for taking a step in the environment, getting available actions and rewards, checking if a state is terminal and printing the agent on the map.\n",
        "*   **DeepQ_Agent class:** contains the Q-learning agent with methods for choosing an action, updating the Q-table (learning), and getting the aspiration.  \n",
        "*   **play function:** Runs iterations of the agent on the environment and updates the Q-table if required. \n",
        "\n",
        "\n",
        "**IN PROGRESS:**\n",
        "*   **Meta-learner:** PyTorch optimizer + module, backpropagation through the Q_table (modelled as a tensor). **Params:** aspiration, weight_1, weight_2. **Loss:** Minimize negative mean cumulative reward \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P630pgO7cYPO",
        "outputId": "bd13918c-1d3c-4372-b183-241996f8adab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dEnZiFSLfiw",
        "outputId": "82ecc921-9743-4cca-8c8a-667e10e2bb4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geneticalgorithm in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from geneticalgorithm) (1.21.6)\n",
            "Requirement already satisfied: func-timeout in /usr/local/lib/python3.8/dist-packages (from geneticalgorithm) (4.3.5)\n"
          ]
        }
      ],
      "source": [
        "pip install geneticalgorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MiWm0wlDhFE"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "from geneticalgorithm import geneticalgorithm as ga\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "height = np.random.randint(5,10)\n",
        "width = np.random.randint(5,10)\n",
        "grid = np.zeros(height * width) -1\n",
        "size = height * width - 1\n",
        "current_location = (np.random.randint(0,size)) #e1\n",
        "bomb_location= np.random.choice([i for i in range(size) if i!= current_location])\n",
        "gold_location= np.random.choice([i for i in range(size) if (i!=current_location and i!= bomb_location)])\n",
        "terminal_states = [bomb_location, gold_location]\n",
        "print(height, width, grid, size, current_location, bomb_location, gold_location, terminal_states)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3-Xre_LM_V4",
        "outputId": "0cdc2e61-6450-48ec-e36f-5120e19048a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9 8 [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.] 71 39 44 10 [44, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM5mEXGRBggC"
      },
      "outputs": [],
      "source": [
        "class DeepGridWorld:\n",
        "    ## Initialise starting data\n",
        "    def __init__(self):\n",
        "        # Set information about the gridworld\n",
        "        self.height = 5\n",
        "        self.width = 5\n",
        "        self.grid = np.zeros(self.height * self.width) -1\n",
        "        self.size = self.height * self.width - 1\n",
        "        \n",
        "        # Set random start location for the agent \n",
        "        self.current_location = (np.random.randint(0,24)) #e1\n",
        "        \n",
        "        # Set locations for the bomb and the gold\n",
        "\n",
        "        #self.bomb_location = np.random.choice([i for i in range(25) if i!=self.current_location]) # e2\n",
        "        #self.gold_location = np.random.choice([i for i in range(25) if (i!=self.current_location and i!=self.bomb_location)]) # e3\n",
        "        \n",
        "        self.bomb_location= 3\n",
        "        self.gold_location= 8\n",
        "        self.terminal_states = [self.bomb_location, self.gold_location]\n",
        "        \n",
        "        # Set grid rewards for special cells\n",
        "        self.grid[self.bomb_location] = -10\n",
        "        self.grid[self.gold_location] = 10\n",
        "        \n",
        "        # Set available actions\n",
        "        self.actions = [0, 1, 2, 3] # e4\n",
        "    \n",
        "        \n",
        "    ## Put methods here:\n",
        "    def get_available_actions(self):\n",
        "        \"\"\"Returns possible actions\"\"\"\n",
        "        return self.actions\n",
        "    \n",
        "    def agent_on_map(self):\n",
        "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
        "        grid = np.zeros(( self.height, self.width))\n",
        "        grid[self.current_location] = 1 #e5\n",
        "        return grid\n",
        "    \n",
        "    def get_reward(self, new_location):\n",
        "        \"\"\"Returns the reward for an input position\"\"\"\n",
        "        return self.grid[new_location] # e6\n",
        "        \n",
        "    \n",
        "    def make_step(self, action):\n",
        "        \"\"\"Moves the agent in the specified direction. If agent is at a border, agent stays still\n",
        "        but takes negative reward. Function returns the reward for the move.\"\"\"\n",
        "        # Store previous location\n",
        "        last_location = self.current_location\n",
        "        \n",
        "        # UP\n",
        "        if action == 0: # e7\n",
        "            # If agent is at the top, stay still, collect reward\n",
        "            if last_location < self.width: #e8 \n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location - self.width) # e9\n",
        "                reward = self.get_reward(self.current_location)\n",
        "        \n",
        "        # DOWN\n",
        "        elif action == 1:\n",
        "            # If agent is at bottom, stay still, collect reward\n",
        "            if last_location >= ((self.width * self.height) - self.width): #e10\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location + self.width) # e11\n",
        "                reward = self.get_reward(self.current_location)\n",
        "            \n",
        "        # LEFT\n",
        "        elif action == 2:\n",
        "            # If agent is at the left, stay still, collect reward\n",
        "            if last_location % self.height == 0: # TODO e12\n",
        "                reward = self.get_reward(last_location) \n",
        "            else:\n",
        "                self.current_location = (self.current_location - 1) # e13\n",
        "                reward = self.get_reward(self.current_location)\n",
        "\n",
        "        # RIGHT\n",
        "        elif action == 3:\n",
        "            # If agent is at the right, stay still, collect reward\n",
        "            if last_location % self.height == self.height - 1: # TODO e14\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                self.current_location = (self.current_location + 1) # e15\n",
        "                reward = self.get_reward(self.current_location)\n",
        "                \n",
        "        return reward\n",
        "    \n",
        "    def check_state(self):\n",
        "        \"\"\"Check if the agent is in a terminal state (gold or bomb), if so return 'TERMINAL'\"\"\"\n",
        "        if self.current_location in self.terminal_states:\n",
        "            return 'TERMINAL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMlAwzH5DbVO"
      },
      "outputs": [],
      "source": [
        "class RandomAgent():        \n",
        "    # Choose a random action\n",
        "    def choose_action(self, available_actions):\n",
        "        \"\"\"Returns a random choice of the available actions\"\"\"\n",
        "        return np.random.choice(available_actions)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WY2L3fbDds8"
      },
      "outputs": [],
      "source": [
        "class DeepQ_Agent(nn.Module):\n",
        "    # Intialise\n",
        "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=0.99, aspiration = 0, weight_1 = 1, weight_2 = 1): # add default aspiration and weights\n",
        "        super().__init__()\n",
        "        self.environment = environment\n",
        "        # self.q_table = dict() # Store all Q-values in dictionary of dictionaries \n",
        "        # for x in range(environment.height): # Loop through all possible grid spaces, create sub-dictionary for each\n",
        "        #     for y in range(environment.width):\n",
        "        #         self.q_table[(x,y)] = {'UP':0, 'DOWN':0, 'LEFT':0, 'RIGHT':0} # Populate sub-dictionary with zero values for possible moves\n",
        "\n",
        "        self.q_table = torch.zeros((environment.height*environment.width, 4), requires_grad=True) #q1\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.aspiration = aspiration\n",
        "        self.weight_1 = weight_1\n",
        "        self.weight_2 = weight_2\n",
        "    \n",
        "    # Choose the action e-greedily \n",
        "    def choose_action(self, available_actions):\n",
        "        \"\"\"Returns the optimal action from Q-Value table. If multiple optimal actions, chooses random choice.\n",
        "        Will make an exploratory random action dependent on epsilon.\"\"\"\n",
        "        if np.random.uniform(0,1) < self.epsilon:\n",
        "            action = available_actions[np.random.randint(0, len(available_actions))]\n",
        "        else:\n",
        "            q_values_of_state = self.q_table[self.environment.current_location] # change to 1D (e1 in env)\n",
        "            maxValue = torch.max(q_values_of_state) # q2\n",
        "            #action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue]) # tricky index of torch.max value and random choice\n",
        "            res = (q_values_of_state == maxValue).nonzero().numpy() # q3 get indices of highest value actions and convert to np array\n",
        "            action = np.random.choice(res[:,0]) # choose the action randomly if there is a tie (slice for 1D array)\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    # Update the Q-table based on the current observation \n",
        "    def learn(self, old_state, reward, new_state, action):\n",
        "        \"\"\"Updates the Q-value table using Q-learning\"\"\"\n",
        "        q_values_of_state = self.q_table[new_state] # index into table self.q_table[] single index numbers not tuples\n",
        "        max_q_value_in_new_state = torch.max(q_values_of_state) # q4\n",
        "        current_q_value = self.q_table[old_state][action]\n",
        "        # define reward with aspiration = w_1 * reward + w_2 * (reward - aspiration)\n",
        "        reward = self.get_aspiration(reward)\n",
        "        self.q_table.data[old_state][action] = (1 - self.alpha) * current_q_value + self.alpha * (reward + self.gamma * max_q_value_in_new_state)\n",
        "    \n",
        "    # Transform the reward function using the aspiration formula #Define aspiration in another way for autograd\n",
        "    def get_aspiration(self, reward):\n",
        "      return self.weight_1 * reward + self.weight_2 * (reward - self.aspiration)\n",
        "    \n",
        "    def play(self,trials=500, max_steps_per_episode=1000, learn=False):\n",
        "      \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "      reward_per_episode = torch.empty(0, requires_grad=True) # Initialise performance log\n",
        "      self.q_table = torch.zeros((environment.height*environment.width, 4)) # reset q table\n",
        "      \n",
        "      for trial in range(trials): # Run trials\n",
        "          cumulative_reward = 0 # Initialise values of each game\n",
        "          step = 0\n",
        "          game_over = False\n",
        "          while step < max_steps_per_episode and game_over != True: # Run until max steps or until game is finished\n",
        "              old_state = self.environment.current_location\n",
        "              action = self.choose_action(self.environment.actions) \n",
        "              reward = self.environment.make_step(action)\n",
        "              new_state = self.environment.current_location\n",
        "              \n",
        "              if learn == True: # Update Q-values if learning is specified\n",
        "                  self.learn(old_state, reward, new_state, action)\n",
        "                  \n",
        "              cumulative_reward += reward\n",
        "              step += 1\n",
        "              \n",
        "              if self.environment.check_state() == 'TERMINAL': # If game is in terminal state, game over and start next trial\n",
        "                  self.environment.__init__()\n",
        "                  game_over = True     \n",
        "                  \n",
        "          reward_per_episode = torch.cat([reward_per_episode, torch.tensor([cumulative_reward])]) # Append reward for current trial to performance log\n",
        "          \n",
        "      return reward_per_episode # Return performance log\n",
        "\n",
        "\n",
        "    \n",
        "    def forward(self,x,trials=10):\n",
        "      # self.aspiration=x[0]\n",
        "      # self.weight_1=x[1]\n",
        "      # self.weight_2=x[2]\n",
        "      #self.aspiration = x\n",
        "      rewards=self.play(trials=trials,learn=True)\n",
        "      #tensor_reward=torch.tensor(rewards,requires_grad=True)\n",
        "     # mean_reward = torch.tensor(torch.mean(tensor_reward), requires_grad=True). #sourceTensor.clone().detach().requires_grad_(True)\n",
        "      return torch.mean(rewards) #return medium rewards \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gbeBfg_hU3K",
        "outputId": "807a3cd7-458b-407f-9094-c15bfe7bf785"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.], grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reward_per_episode = torch.zeros(1, requires_grad=True)\n",
        "reward_per_episode.add(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeqM3aSVDqFg"
      },
      "outputs": [],
      "source": [
        "def play(environment, agent, trials=500, max_steps_per_episode=1000, learn=False):\n",
        "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "    reward_per_episode = [] # Initialise performance log\n",
        "    \n",
        "    for trial in range(trials): # Run trials\n",
        "        cumulative_reward = 0 # Initialise values of each game\n",
        "        step = 0\n",
        "        game_over = False\n",
        "        while step < max_steps_per_episode and game_over != True: # Run until max steps or until game is finished\n",
        "            old_state = environment.current_location\n",
        "            action = agent.choose_action(environment.actions) \n",
        "            reward = environment.make_step(action)\n",
        "            new_state = environment.current_location\n",
        "            \n",
        "            if learn == True: # Update Q-values if learning is specified\n",
        "                agent.learn(old_state, reward, new_state, action)\n",
        "                \n",
        "            cumulative_reward += reward\n",
        "            step += 1\n",
        "            \n",
        "            if environment.check_state() == 'TERMINAL': # If game is in terminal state, game over and start next trial\n",
        "                environment.__init__()\n",
        "                game_over = True     \n",
        "                \n",
        "        reward_per_episode.append(cumulative_reward) # Append reward for current trial to performance log\n",
        "        \n",
        "    return reward_per_episode # Return performance log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cQsCAWbbFDp"
      },
      "source": [
        "### Testing Torch version of play for autograd:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jit7BVRQZcXM"
      },
      "outputs": [],
      "source": [
        "# def new_play(environment, agent,trials=500, max_steps_per_episode=1000, learn=False):\n",
        "#       \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "#       reward_per_episode = torch.empty(0, requires_grad=True) # Initialise performance log\n",
        "      \n",
        "#       for trial in range(trials): # Run trials\n",
        "#           cumulative_reward = 0 # Initialise values of each game\n",
        "#           step = 0\n",
        "#           game_over = False\n",
        "#           while step < max_steps_per_episode and game_over != True: # Run until max steps or until game is finished\n",
        "#               old_state = environment.current_location\n",
        "#               action = agent.choose_action(environment.actions) \n",
        "#               reward = environment.make_step(action)\n",
        "#               new_state = environment.current_location\n",
        "              \n",
        "#               if learn == True: # Update Q-values if learning is specified\n",
        "#                   agent.learn(old_state, reward, new_state, action)\n",
        "                  \n",
        "#               cumulative_reward += reward\n",
        "#               step += 1\n",
        "              \n",
        "#               if environment.check_state() == 'TERMINAL': # If game is in terminal state, game over and start next trial\n",
        "#                   environment.__init__()\n",
        "#                   game_over = True     \n",
        "                  \n",
        "#           reward_per_episode = torch.cat([reward_per_episode, torch.tensor([cumulative_reward])]) # Append reward for current trial to performance log\n",
        "          \n",
        "#       return reward_per_episode # Return performance log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "-tDNCiPTDqxd",
        "outputId": "b95c9e06-57ee-4c33-e189-6b74e65a634b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b48f22123b2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mav_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mreward_per_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#plt.plot(reward_per_episode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-197167c379d2>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(environment, agent, trials, max_steps_per_episode, learn)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Update Q-values if learning is specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mcumulative_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8c58ec0780e7>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, old_state, reward, new_state, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# define reward with aspiration = w_1 * reward + w_2 * (reward - aspiration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_aspiration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_q_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q_value_in_new_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Transform the reward function using the aspiration formula #Define aspiration in another way for autograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "environment = DeepGridWorld()\n",
        "agent = DeepQ_Agent(environment, aspiration = 0)\n",
        "av_reward = 0 \n",
        "for iteration in range(10):\n",
        "  reward_per_episode = play(environment, agent, trials = 500, learn = True)\n",
        "\n",
        "  #plt.plot(reward_per_episode)\n",
        "  av_reward += sum(reward_per_episode) \n",
        "\n",
        "print(av_reward/10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiFoTCYMOEGz"
      },
      "source": [
        "### Testing aspiration (100 iterations, 500 trials)  \n",
        "2184.75, aspiration = 0.5 \\\\\n",
        "2182.38       , aspiration = 0.2 \\\\\n",
        "2195.28      , aspiration = 0.08 \\\\\n",
        "2181.2      , aspiration = 0.05 \\\\\n",
        "2188.52, aspiration = 0 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "eu1QVXR7EEq-",
        "outputId": "fbab021c-83c5-4e62-b22c-12905cf23e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25, 4])\n",
            "7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "environment = DeepGridWorld()\n",
        "agent = DeepQ_Agent(environment, aspiration = 0.20173071)\n",
        "av_reward = 0 \n",
        "print(agent.q_table.shape)\n",
        "reward_per_episode = agent.play( learn = True)\n",
        "\n",
        "plt.plot(reward_per_episode.detach().numpy())\n",
        "#print(reward_per_episode)\n",
        "print(np.mean(reward_per_episode.detach().numpy()[-1:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nLNe1wnZfh8",
        "outputId": "59b87974-ecd8-4a40-c0cf-7ea145b9f471"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "means = []\n",
        "\n",
        "for it in range(100):\n",
        "  environment = DeepGridWorld()\n",
        "  agent = DeepQ_Agent(environment, aspiration = 0.20173071)\n",
        "  reward_per_episode = play(environment, agent, trials=100, learn = True)\n",
        "  means.append(reward_per_episode[-15:])\n",
        "\n",
        "np.mean(means)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsqjWPuMMIa_"
      },
      "source": [
        "### Testing Evolutionary Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "J-TE1R7SMH9Z",
        "outputId": "cb5e4f89-1559-4510-f8e6-5089ae3edb35"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3c1ded564b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mga\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvariable_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'real'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvariable_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvarbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# optimal = 0.20173071\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/geneticalgorithm/geneticalgorithm.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0msolo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/geneticalgorithm/geneticalgorithm.py\u001b[0m in \u001b[0;36msim\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuntimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFunctionTimedOut\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"given function is not applicable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/func_timeout/dafunc.py\u001b[0m in \u001b[0;36mfunc_timeout\u001b[0;34m(timeout, func, args, kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/func_timeout/py3_raise.py\u001b[0m in \u001b[0;36mraise_exception\u001b[0;34m(exception)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Only available in python3.3+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/geneticalgorithm/geneticalgorithm.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-3c1ded564b9d>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(aspiration)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepQ_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspiration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maspiration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mreward_per_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_per_episode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c69447213b9a>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, trials, max_steps_per_episode, learn)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Update Q-values if learning is specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m               \u001b[0mcumulative_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-c69447213b9a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, old_state, reward, new_state, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# define reward with aspiration = w_1 * reward + w_2 * (reward - aspiration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_aspiration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_q_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q_value_in_new_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Transform the reward function using the aspiration formula #Define aspiration in another way for autograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations."
          ]
        }
      ],
      "source": [
        "def optimize(aspiration):\n",
        "    environment = DeepGridWorld()\n",
        "    agent = DeepQ_Agent(environment,aspiration = aspiration)\n",
        "    reward_per_episode = agent.play(learn = True)\n",
        "    return -np.mean(reward_per_episode.detach().numpy()[-15:])\n",
        "    \n",
        "varbound=np.array([[0,1]])\n",
        "\n",
        "algorithm_param = {'max_num_iteration': 100,\\\n",
        "                   'population_size':100,\\\n",
        "                   'mutation_probability':0.1,\\\n",
        "                   'elit_ratio': 0.01,\\\n",
        "                   'crossover_probability': 0.5,\\\n",
        "                   'parents_portion': 0.3,\\\n",
        "                   'crossover_type':'uniform',\\\n",
        "                   'max_iteration_without_improv':None}\n",
        "\n",
        "model=ga(function= optimize,dimension=1,variable_type='real',variable_boundaries=varbound, algorithm_parameters = algorithm_param)\n",
        "\n",
        "model.run() # optimal = 0.20173071"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnW3yEC8bNVe"
      },
      "source": [
        "### Testing the Optimizer which acts as a meta-learner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "4o8AHjxYd9TS",
        "outputId": "2bc3ab16-fd4c-4007-f8c7-21a55d24ea30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-cac5aa2e5f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mouter_trial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mmean_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-b1d87ceff230>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, trials)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;31m# self.weight_2=x[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;31m#self.aspiration = x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m       \u001b[0;31m#tensor_reward=torch.tensor(rewards,requires_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m      \u001b[0;31m# mean_reward = torch.tensor(torch.mean(tensor_reward), requires_grad=True). #sourceTensor.clone().detach().requires_grad_(True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-b1d87ceff230>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, trials, max_steps_per_episode, learn)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Update Q-values if learning is specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m               \u001b[0mcumulative_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-b1d87ceff230>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, old_state, reward, new_state, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mcurrent_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# define reward with aspiration = w_1 * reward + w_2 * (reward - aspiration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_aspiration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurrent_q_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q_value_in_new_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-b1d87ceff230>\u001b[0m in \u001b[0;36mget_aspiration\u001b[0;34m(self, reward)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Transform the reward function using the aspiration formula #Define aspiration in another way for autograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_aspiration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspiration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps_per_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "params=nn.Parameter(torch.tensor([0.1], requires_grad=True))\n",
        "optimizer=torch.optim.SGD([agent.aspiration],lr=0.1,momentum=0.9)\n",
        "environment = DeepGridWorld()\n",
        "for outer_trial in range(10):\n",
        "  mean_reward=agent(params,trials=500)\n",
        "  loss=-mean_reward\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step() \n",
        "  print(agent.aspiration)\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9-4Q8YehjDu",
        "outputId": "ec6ef709-b029-489b-f241-5f14e8a6a9d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1.0000, 1.0000, 0.0500], grad_fn=<StackBackward0>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight_1=nn.Parameter(torch.tensor(1.0))\n",
        "weight_2=nn.Parameter(torch.tensor(1.0))\n",
        "aspiration=nn.Parameter(torch.tensor(0.05))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "wdEcqT7ehvBg",
        "outputId": "88ffe289-a662-42fc-ab17-f77e6041b135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([0.1000], requires_grad=True)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-3abf6fc059b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mmean_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mnegative_mean_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mnegative_mean_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-b1d87ceff230>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, trials)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;31m# self.weight_2=x[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;31m#self.aspiration = x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m       \u001b[0;31m#tensor_reward=torch.tensor(rewards,requires_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m      \u001b[0;31m# mean_reward = torch.tensor(torch.mean(tensor_reward), requires_grad=True). #sourceTensor.clone().detach().requires_grad_(True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-b1d87ceff230>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, trials, max_steps_per_episode, learn)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Update Q-values if learning is specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m               \u001b[0mcumulative_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-b1d87ceff230>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, old_state, reward, new_state, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mq_values_of_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# index into table self.q_table[] single index numbers not tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmax_q_value_in_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values_of_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# q4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mcurrent_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# define reward with aspiration = w_1 * reward + w_2 * (reward - aspiration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_aspiration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "params=nn.Parameter(torch.tensor([0.0]),requires_grad=True)\n",
        "optimizer=torch.optim.Adam([params], lr = 0.1)\n",
        "environment = DeepGridWorld()\n",
        "for outer_trial in range(10):\n",
        "  environment.__init__()\n",
        "  optimizer.zero_grad()\n",
        "  mean_reward=agent(params,trials=500)\n",
        "  negative_mean_reward=-mean_reward\n",
        "  negative_mean_reward.backward()\n",
        "  optimizer.step() \n",
        "  print(agent.aspiration)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ElgPf78LAwU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define a PyTorch model for the gamma value\n",
        "class GammaModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GammaModel, self).__init__()\n",
        "    self.gamma = torch.nn.Parameter(torch.tensor(0.5))\n",
        "  \n",
        "  def forward(self):\n",
        "    return self.gamma\n",
        "\n",
        "# Define a function for evaluating the performance of the gamma model\n",
        "def evaluate_gamma_model(model, env, agent, num_episodes):\n",
        "  total_reward = 0\n",
        "  for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = agent.act(state, gamma=model())\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      total_reward += reward\n",
        "      state = next_state\n",
        "  return total_reward\n",
        "\n",
        "# Define a function for training the gamma model\n",
        "def train_gamma_model(model, optimizer, env, agent, num_episodes, num_iterations):\n",
        "  for i in range(num_iterations):\n",
        "    optimizer.zero_grad()\n",
        "    reward = evaluate_gamma_model(model, env, agent, num_episodes)\n",
        "    loss = -reward  # maximize reward by minimizing the negative reward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Create the gamma model and optimizer\n",
        "model = GammaModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the gamma model\n",
        "train_gamma_model(model, optimizer, env, agent, num_episodes=100, num_iterations=100)\n",
        "\n",
        "# The trained model's gamma value is the optimal gamma value\n",
        "optimal_gamma = model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}